{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "* Training a naive bayes model on a sentiment analysis task\n",
    "* Test using your model\n",
    "* Compute ratios of positive words to negative words\n",
    "* Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "## Importing Functions and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/user/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pdb\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from os import getcwd\n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exampl', 'tweet', 'stop', 'word', 'variou', 'word', 'form']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    # Tokenize the tweet into words\n",
    "    words = word_tokenize(tweet)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Apply stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "    \n",
    "    # Remove punctuation\n",
    "    no_punct_words = [word for word in stemmed_words if word not in string.punctuation]\n",
    "    \n",
    "    return no_punct_words\n",
    "\n",
    "# Example usage\n",
    "tweet = \"This is an example tweet with some stop words and various word forms.\"\n",
    "processed_words = process_tweet(tweet)\n",
    "print(processed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sets of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# split the data into two pieces, one for training and one for testing (validation set)\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "# avoid assumptions about the length of all_positive_tweets\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Process the Data\n",
    "\n",
    "`process_tweet`  does this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rt', 'twitter', 'chapagain', 'hello', 'great', 'day', 'good', 'morn', 'http', '//chapagain.com.np']\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "\n",
    "# print cleaned tweet\n",
    "print(process_tweet(custom_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_tweets\n",
    "\n",
    "def count_tweets(result, tweets, ys):\n",
    "    '''\n",
    "    Input:\n",
    "        result: a dictionary that will be used to map each pair to its frequency\n",
    "        tweets: a list of tweets\n",
    "        ys: a list corresponding to the sentiment of each tweet (either 0 or 1)\n",
    "    Output:\n",
    "        result: a dictionary mapping each pair to its frequency\n",
    "    '''\n",
    "    for y, tweet in zip(ys, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            # define the key, which is the word and label tuple\n",
    "            pair = (word, y)\n",
    "            \n",
    "            # if the key exists in the dictionary, increment the count\n",
    "            if pair in result:\n",
    "                result[pair] += 1\n",
    "\n",
    "            # else, if the key is new, add it to the dictionary and set the count to 1\n",
    "            else:\n",
    "                result[pair] = 1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing your function\n",
    "\n",
    "result = {}\n",
    "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
    "ys = [1, 0, 0, 0, 0]\n",
    "count_tweets(result, tweets, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Training Naive Bayes\n",
    "\n",
    "Naive bayes is an algorithm that could be used for sentiment analysis. It takes a short time to train and also has a short prediction time.\n",
    "\n",
    "$P(D_{pos})$ is the probability that the document is positive.\n",
    "$P(D_{neg})$ is the probability that the document is negative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior and Logprior\n",
    "\n",
    "The prior probability represents the underlying probability in the target population that a tweet is positive versus negative.  In other words, if we had no specific information and blindly picked a tweet out of the population set, what is the probability that it will be positive versus that it will be negative? That is the \"prior\".\n",
    "\n",
    "The prior is the ratio of the probabilities $\\frac{P(D_{pos})}{P(D_{neg})}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positive and Negative Probability of a Word\n",
    "- $freq_{pos}$ and $freq_{neg}$ are the frequencies of that specific word in the positive or negative class. In other words, the positive frequency of a word is the number of times the word is counted with the label of 1.\n",
    "- $N_{pos}$ and $N_{neg}$ are the total number of positive and negative words for all documents (for all tweets), respectively.\n",
    "- $V$ is the number of unique words in the entire set of documents, for all classes, whether positive or negative.\n",
    "\n",
    "\n",
    "$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n",
    "$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n",
    "\n",
    "Add the \"+1\" for additive smoothing.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log likelihood\n",
    "\n",
    "$$\\text{loglikelihood} = \\log \\left(\\frac{P(W_{pos})}{P(W_{neg})} \\right)\\tag{6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create `freqs` dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the freqs dictionary for later uses\n",
    "freqs = count_tweets({}, train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - train_naive_bayes\n",
    "Given a freqs dictionary, `train_x` (a list of tweets) and a `train_y` (a list of labels for each tweet), implement a naive bayes classifier.\n",
    "\n",
    "- Calculate $V$\n",
    "- Calculate $freq_{pos}$ and $freq_{neg}$\n",
    "- Calculate $N_{pos}$, and $N_{neg}$\n",
    "- Calculate $D$, $D_{pos}$, $D_{neg}$\n",
    "- Calculate the logprior\n",
    "- Calculate log likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary from (word, label) to how often the word appears\n",
    "        train_x: a list of tweets\n",
    "        train_y: a list of labels correponding to the tweets (0,1)\n",
    "    Output:\n",
    "        logprior: the log prior. (equation 3 above)\n",
    "        loglikelihood: the log likelihood of your Naive Bayes equation. (equation 6 above)\n",
    "    '''\n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "\n",
    "\n",
    "    # Calculate V, the number of unique words in the vocabulary\n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "\n",
    "    # Calculate N_pos, N_neg, V_pos, V_neg\n",
    "    N_pos = N_neg = 0\n",
    "    for pair in freqs.keys():\n",
    "        if pair[1] > 0:\n",
    "            N_pos += freqs[pair]\n",
    "        else:\n",
    "            N_neg += freqs[pair]\n",
    "    \n",
    "    # Calculate D, the number of documents\n",
    "    D = len(train_y)\n",
    "\n",
    "    # Calculate D_pos, the number of positive documents\n",
    "    D_pos = sum(train_y)\n",
    "\n",
    "    # Calculate D_neg, the number of negative documents\n",
    "    D_neg = D - D_pos\n",
    "\n",
    "    # Calculate logprior\n",
    "    logprior = np.log(D_pos) - np.log(D_neg)\n",
    "    \n",
    "    # For each word in the vocabulary...\n",
    "    for word in vocab:\n",
    "        # Get the positive and negative frequency of the word\n",
    "        freq_pos = freqs.get((word, 1), 0)\n",
    "        freq_neg = freqs.get((word, 0), 0)\n",
    "\n",
    "        # Calculate the probability that each word is positive, and negative\n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
    "\n",
    "        # Calculate the log likelihood of the word\n",
    "        loglikelihood[word] = np.log(p_w_pos / p_w_neg)\n",
    "\n",
    "\n",
    "    return logprior, loglikelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "16310\n"
     ]
    }
   ],
   "source": [
    "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
    "print(logprior)\n",
    "print(len(loglikelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Testing\n",
    "\n",
    "Now that we have the `logprior` and `loglikelihood`, we can test the naive bayes function by making predicting on some tweets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive_bayes_predict\n",
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string\n",
    "        logprior: a number\n",
    "        loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
    "    '''\n",
    "    ### START CODE HERE ###\n",
    "    # Process the tweet to get a list of words\n",
    "    word_l = process_tweet(tweet)\n",
    "\n",
    "    # Initialize probability to zero\n",
    "    p = 0.0\n",
    "\n",
    "    # Add the logprior\n",
    "    p += logprior\n",
    "\n",
    "    for word in word_l:\n",
    "        # Check if the word exists in the loglikelihood dictionary\n",
    "        if word in loglikelihood:\n",
    "            # Add the log likelihood of that word to the probability\n",
    "            p += loglikelihood[word]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected output is 1.4568612465665989\n"
     ]
    }
   ],
   "source": [
    "my_tweet = 'She smiled.'\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print('The expected output is', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected output is -0.4056672935496634\n"
     ]
    }
   ],
   "source": [
    "# Experiment with your own tweet.\n",
    "my_tweet = 'He laughed.'\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print('The expected output is', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_naive_bayes\n",
    "\n",
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood, naive_bayes_predict=naive_bayes_predict):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_x: A list of tweets\n",
    "        test_y: the corresponding labels for the list of tweets\n",
    "        logprior: the logprior\n",
    "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
    "    Output:\n",
    "        accuracy: (# of tweets classified correctly)/(total # of tweets)\n",
    "    \"\"\"\n",
    "    accuracy = 0  # return this properly\n",
    "\n",
    "    y_hats = []\n",
    "    for tweet in test_x:\n",
    "        # If the prediction is > 0\n",
    "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
    "            # The predicted class is 1\n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "            # Otherwise, the predicted class is 0\n",
    "            y_hat_i = 0\n",
    "\n",
    "        # Append the predicted class to the list y_hats\n",
    "        y_hats.append(y_hat_i)\n",
    "\n",
    "    # Error is the average of the absolute values of the differences between y_hats and test_y\n",
    "    error = np.mean(np.abs(y_hats - test_y))\n",
    "\n",
    "    # Accuracy is 1 minus the error\n",
    "    accuracy = 1 - error\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy = 0.7640\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes accuracy = %0.4f\" %\n",
    "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy -> 2.08\n",
      "I am bad -> -1.25\n",
      "this movie should have been great. -> 2.00\n",
      "great -> 2.07\n",
      "great great -> 4.14\n",
      "great great great -> 6.21\n",
      "great great great great -> 8.28\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to test your function\n",
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
    "    # print( '%s -> %f' % (tweet, naive_bayes_predict(tweet, logprior, loglikelihood)))\n",
    "    p = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
    "#     print(f'{tweet} -> {p:.2f} ({p_category})')\n",
    "    print(f'{tweet} -> {p:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Filter words by Ratio \n",
    "\n",
    "Implement get_ratio.\n",
    "\n",
    "- Given the freqs dictionary of words and a particular word, use `lookup(freqs,word,1)` to get the positive count of the word.\n",
    "- Similarly, use the `lookup` function to get the negative count of that word.\n",
    "- Calculate the ratio of positive divided by negative counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup(freqs, word, label):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary containing word frequency counts\n",
    "        word: the word you want to look up\n",
    "        label: the class label (0 for negative, 1 for positive)\n",
    "    Output:\n",
    "        count: the frequency count of the specified word and label\n",
    "    '''\n",
    "    key = (word, label)\n",
    "    if key in freqs:\n",
    "        count = freqs[key]\n",
    "    else:\n",
    "        count = 0\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ratio\n",
    "\n",
    "def get_ratio(freqs, word):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary containing the words\n",
    "\n",
    "    Output: a dictionary with keys 'positive', 'negative', and 'ratio'.\n",
    "        Example: {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
    "    '''\n",
    "    pos_neg_ratio = {'positive': 0, 'negative': 0, 'ratio': 0.0}\n",
    "    ### START CODE HERE ###\n",
    "    # use lookup() to find positive counts for the word (denoted by the integer 1)\n",
    "    pos_neg_ratio['positive'] = freqs.get((word, 1), 0)\n",
    "    \n",
    "    # use lookup() to find negative counts for the word (denoted by integer 0)\n",
    "    pos_neg_ratio['negative'] = freqs.get((word, 0), 0)\n",
    "    \n",
    "    # calculate the ratio of positive to negative counts for the word\n",
    "    pos_neg_ratio['ratio'] = (pos_neg_ratio['positive']+1) / (pos_neg_ratio['negative'] + 1)\n",
    "    ### END CODE HERE ###\n",
    "    return pos_neg_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': 162, 'negative': 18, 'ratio': 8.578947368421053}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ratio(freqs, 'happi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_words_by_threshold\n",
    "\n",
    "def get_words_by_threshold(freqs, label, threshold, get_ratio=get_ratio):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary of words\n",
    "        label: 1 for positive, 0 for negative\n",
    "        threshold: ratio that will be used as the cutoff for including a word in the returned dictionary\n",
    "    Output:\n",
    "        word_list: dictionary containing the word and information on its positive count, negative count, and ratio of positive to negative counts.\n",
    "        example of a key value pair:\n",
    "        {'happi':\n",
    "            {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
    "        }\n",
    "    '''\n",
    "    word_list = {}\n",
    "\n",
    "    for key in freqs.keys():\n",
    "        word, _ = key\n",
    "\n",
    "        # get the positive/negative ratio for a word\n",
    "        pos_neg_ratio = get_ratio(freqs, word)\n",
    "\n",
    "        # if the label is 1 and the ratio is greater than or equal to the threshold...\n",
    "        if label == 1 and pos_neg_ratio['ratio'] >= threshold:\n",
    "        \n",
    "            # Add the pos_neg_ratio to the dictionary\n",
    "            word_list[word] = pos_neg_ratio\n",
    "\n",
    "        # If the label is 0 and the pos_neg_ratio is less than or equal to the threshold...\n",
    "        elif label == 0 and pos_neg_ratio['ratio'] <= threshold:\n",
    "        \n",
    "            # Add the pos_neg_ratio to the dictionary\n",
    "            word_list[word] = pos_neg_ratio\n",
    "\n",
    "        # otherwise, do not include this word in the list (do nothing)\n",
    "\n",
    "    return word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'followfriday': {'positive': 23, 'negative': 0, 'ratio': 24.0},\n",
       " 'bhaktisbant': {'positive': 16, 'negative': 0, 'ratio': 17.0},\n",
       " 'flipkartfashionfriday': {'positive': 16, 'negative': 0, 'ratio': 17.0},\n",
       " 'p': {'positive': 107, 'negative': 1, 'ratio': 54.0},\n",
       " 'influenc': {'positive': 16, 'negative': 0, 'ratio': 17.0},\n",
       " 'jnlazt': {'positive': 62, 'negative': 0, 'ratio': 63.0},\n",
       " '//t.co/rcvcyyo0iq': {'positive': 62, 'negative': 0, 'ratio': 63.0},\n",
       " 'youth': {'positive': 15, 'negative': 0, 'ratio': 16.0},\n",
       " 'tolajobjob': {'positive': 14, 'negative': 0, 'ratio': 15.0},\n",
       " 'bam': {'positive': 44, 'negative': 0, 'ratio': 45.0},\n",
       " 'barsandmelodi': {'positive': 44, 'negative': 0, 'ratio': 45.0},\n",
       " '969horan696': {'positive': 44, 'negative': 0, 'ratio': 45.0},\n",
       " 'warsaw': {'positive': 44, 'negative': 0, 'ratio': 45.0},\n",
       " 'stat': {'positive': 51, 'negative': 0, 'ratio': 52.0},\n",
       " 'impastel': {'positive': 17, 'negative': 0, 'ratio': 18.0},\n",
       " 'blog': {'positive': 27, 'negative': 0, 'ratio': 28.0},\n",
       " 'fback': {'positive': 26, 'negative': 0, 'ratio': 27.0}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function; find positive words at or above a threshold\n",
    "get_words_by_threshold(freqs, label=1, threshold=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Error Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth Predicted Tweet\n",
      "1\t0.00\tb'bro u wan cut hair anot ur hair long liao bo sinc ord liao take easi lor treat save leav longer bro lol sibei xialan'\n",
      "1\t0.00\tb\"humayag 'stuck centr right clown right joker left ... orgasticpot ahmedshahe ahmedsaeedgahaa\"\n",
      "1\t0.00\tb'tim_a_robert pinter_quot work'\n",
      "1\t0.00\tb'sasarichardson stefbystef_ frgt10_anthem hahahahahahahahahahahahahaha dy liter front like'\n",
      "1\t0.00\tb\"'s awak\"\n",
      "1\t0.00\tb'charlesjonesss f'\n",
      "1\t0.00\tb\"scooterblue1962 thank ye let 's hope work miss\"\n",
      "1\t0.00\tb\"awkward moment name 'akarshan end stay 'singl foreveralon\"\n",
      "1\t0.00\tb'v4violetta highfiv probabl ahead sinc less artsi verbal'\n",
      "1\t0.00\tb'dat rp tho thank much guy celebr one month partnership ty madmorphtv raid'\n",
      "1\t0.00\tb'seniorspazz tehsmiley bore everyth'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t0.00\tb'jarednotsubway iluvmariah bravotv truli later move know queen bee upward bound movingonup'\n",
      "1\t0.00\tb'caballeroserena actual need stop tweet drive'\n",
      "1\t0.00\tb\"well 's littlemix presal ticket bought thank ticketmasteruk wonder take book ... thing parent\"\n",
      "1\t0.00\tb'betcha dumb butt'\n",
      "1\t0.00\tb'kik qualky808 kik kikmenow milf like4lik bore summer sexysaturday http //t.co/8r2nrl31ic'\n",
      "1\t0.00\tb\"madison420ivi `` '' wish kid\"\n",
      "1\t0.00\tb'markbreech sure would good thing 4 bottom dare 2 say 2 miss b im gon na stubborn mouth soap nothavingit p'\n",
      "1\t0.00\tb\"saharjojo10 ye n't car\"\n",
      "1\t0.00\tb'thewhitespik like especi klee one'\n",
      "1\t0.00\tb'catargiu yeah kinda feel like warm butter'\n",
      "1\t0.00\tb'madpilot'\n",
      "1\t0.00\tb'whenev sister see cri text ask im okay aw someon care'\n",
      "1\t0.00\tb\"sisiphomphoza ca n't wait see\"\n",
      "1\t0.00\tb'shadypenguinn take care'\n",
      "1\t0.00\tb'waitlesscompani aledeleonmoreno glyon'\n",
      "1\t0.00\tb\"andyherren know dumb think johnni rock u seen utub video 's mad respect\"\n",
      "1\t0.00\tb'imtoxic21 sorri loss hope goe well'\n",
      "1\t0.00\tb'kezhoskyn got call end pigeon home safe definit worth contact'\n",
      "1\t0.00\tb'morn hospit day today hope final get sign long year pain op fingerscross'\n",
      "1\t0.00\tb'frolichawaii saturday'\n",
      "1\t0.00\tb'look fun kik thencerest547 kik kiksext sex followback l4l indiemus kikgirl http //t.co/rgxnoghmkm'\n",
      "1\t0.00\tb'matjazsircelj ftw'\n",
      "1\t0.00\tb'lovingjeonboram ah see song prefer take maman'\n",
      "1\t0.00\tb\"'s like hurt feel anyth right\"\n",
      "1\t0.00\tb'check new neonarena van outsid olymp park uk_sport anniversari game http //t.co/6qrbnfxzg7'\n",
      "1\t0.00\tb\"clareyh yeah 's classi look 've seen peopl attach sort 'life save equip bed fix -0\"\n",
      "1\t0.00\tb'trend .. follow bobbl link http //t.co/hzxoire88h'\n",
      "1\t0.00\tb\"next year sure though 's 2 girl piss\"\n",
      "1\t0.00\tb'gotzefi im tri d/l mh3 english patch psp'\n",
      "1\t0.00\tb\"want tweet someth thought 'll probabl lose lot liam girl mutual wo n't\"\n",
      "1\t0.00\tb'h_eartshapedbox first leg magictrikband tour go well music band rock magictrik tour  http //t.co/3sdqjnwdfm'\n",
      "1\t0.00\tb\"whatsapp roommat '' want anyth pari '' `` french man '' lol sure 'll head park grab one\"\n",
      "1\t0.00\tb'idxterr b3dk far7an ank mis15'\n",
      "1\t0.00\tb\"woke feel incred sick idk 's caus drank starbuck 11 o'clock last night 's reaction med\"\n",
      "1\t0.00\tb'call night go sleep'\n",
      "1\t0.00\tb'charliehaard wealilknowa elliotpend j_mezzer smile receiv text dad say suck'\n",
      "1\t0.00\tb'mussshiii takafofo wat'\n",
      "1\t0.00\tb\"mum 's home .... hahahhaah thank lord\"\n",
      "1\t0.00\tb'_sw_e_ safe'\n",
      "1\t0.00\tb\"sasarichardson stefbystef_ frgt10_anthem sombrero high okay 'm lost http //t.co/2vdpgb5epk\"\n",
      "1\t0.00\tb'mariaburtea 5sos_nightli wow realli hate'\n",
      "1\t0.00\tb'floreani_mgmt danolsenmus ukopenm bbc3 watch saw dan'\n",
      "1\t0.00\tb\"want protect relationship other 'll talk ppl n't need know awesom\"\n",
      "1\t0.00\tb\"theavianfurri oh dang zen look `` outsid '' space neat\"\n",
      "1\t0.00\tb\"arronjones31 hiya 'm afraid 've alreadi pick rider year 'll look next year\"\n",
      "1\t0.00\tb'aaliyan_ lucki'\n",
      "1\t0.00\tb'new report talk burn calori cold work harder warm feel better weather p'\n",
      "1\t0.00\tb\"say someth 'm give 'm sorri could n't get\"\n",
      "1\t0.00\tb\"heyoppar zain9898 bemybelief hetthuocchua 's gon na anoth one final\"\n",
      "1\t0.00\tb\"manningoffici garrjprbdf psychedk 'm sure tapir calf n't one like adventur mealtim\"\n",
      "1\t0.00\tb'yumnoskians_x yeah tri look x'\n",
      "1\t0.00\tb'katie_taylorkay ye omg lile tid tmi'\n",
      "1\t0.00\tb\"vincenttong007 ca n't wait week\"\n",
      "1\t0.00\tb'shobanam person small thing u expect person u love'\n",
      "1\t0.00\tb'tbhhowel oh shoot well watch'\n",
      "1\t0.00\tb'anyi_anonadada jospastr norcoreano tope'\n",
      "1\t0.00\tb\"hey fam vote 5so n't alreadi got ta win boy teenchoic -s http //t.co/zbzvxc0v5i\"\n",
      "1\t0.00\tb\"ha almost gave fuck boy trash wast time 's sure\"\n",
      "1\t0.00\tb'thaque883 noth much need grow'\n",
      "1\t0.00\tb'catch fall fall ontrack'\n",
      "1\t0.00\tb'briones198 alway make thing better '\n",
      "1\t0.00\tb\"cat_grl uberlinco guess 'll show german also subtitl\"\n",
      "1\t0.00\tb'bekhyungh cunyelo pinter'\n",
      "1\t0.00\tb'rachelburden fabul time miss xx'\n",
      "1\t0.00\tb'pannusf10 never show much'\n",
      "1\t0.00\tb\"sennicka n't engin\"\n",
      "1\t0.00\tb'7'\n",
      "1\t0.00\tb'req edsheeran star solo_radio claraapuspita fridayloug'\n",
      "1\t0.00\tb'knew whole class cri hurt word'\n",
      "1\t0.00\tb'lunch imperiallamian'\n",
      "1\t0.00\tb\"_stevievi 'll hawaii decemb 'll kick back miss\"\n",
      "1\t0.00\tb\"themaddiebruc nah 're beaut way\"\n",
      "1\t0.00\tb'lusciouslyndee1 hahaha ... thu ... sneak away log niteflirt'\n",
      "1\t0.00\tb\"wontanim yep 're trash af im indonesia hbu\"\n",
      "1\t0.00\tb'dbuzzketbal dbuzzketbal say must chri elli'\n",
      "1\t0.00\tb\"'s remind need one http //t.co/iwlqk3xb3h\"\n",
      "1\t0.00\tb\"harri niall -94 harri born ik 's stupid wan na chang http //t.co/ghat8zdaff\"\n",
      "1\t0.00\tb\"surreyhillsbrew ... play al n 'll score fuck night\"\n",
      "1\t0.00\tb'im send alex like million pictur'\n",
      "1\t0.00\tb\"cat_grl berlindisast boom 's date `` z\"\n",
      "1\t0.00\tb'19murf85 that way see west ham shit small club london villa 3rd biggest brum'\n",
      "1\t0.00\tb'sweetiebellaxx deni vagina rli realdstoff poor thing'\n",
      "1\t0.00\tb\"flimflammeri disneylanddtd oh white rabbit cutest thing 've ever seen wait 's ador\"\n",
      "1\t0.00\tb'alway motiv'\n",
      "1\t0.00\tb\"hellojennyho haha 's kyle gf 's babi\"\n",
      "1\t0.00\tb'goooood mooorn want sleep ...'\n",
      "1\t0.00\tb'lose'\n",
      "1\t0.00\tb'fni re-sign'\n",
      "1\t0.00\tb\"meet harri 's need life\"\n",
      "1\t0.00\tb'one sleep til wed'\n",
      "1\t0.00\tb'hattersleyblue3'\n",
      "1\t0.00\tb\"ricky27_263 remedi media wo n't tell\"\n",
      "1\t0.00\tb'grafikmag editionsdulivr kid insid want one'\n",
      "1\t0.00\tb'sorri fuck eric think he funni'\n",
      "1\t0.00\tb'pimpl forehead size volcano'\n",
      "1\t0.00\tb'back done na mag miryenda'\n",
      "1\t0.00\tb'nd go expir next year dat mtn 6gb u realli like'\n",
      "1\t0.00\tb'wan na go back time everyth still fine'\n",
      "1\t0.00\tb'feel like relaps'\n",
      "1\t0.00\tb'waglington finish mass effect 1 yesterday dont know shoud start mass effect 2 100 game'\n",
      "1\t0.00\tb'bad would remind exercis 1:12 miss need come back'\n",
      "1\t0.00\tb'add kik ughtmed545 kik kikmeguy kissm nude likeforfollow musicbiz sexysasunday http //t.co/bgtjjz7ffn'\n",
      "1\t0.00\tb'snapchat sexyjudy19 snapchat kikmeboy tagsforlik pussi gay indiemus sexo http //t.co/kzdarkkieg'\n",
      "1\t0.00\tb'creeperfartss come chill fire'\n",
      "1\t0.00\tb'yknoc monte/doa header'\n",
      "1\t0.00\tb'50pip ted speaker say stress good bodi anoth speaker introduc way avoid stress .ted=thought laboratori'\n",
      "1\t0.00\tb'ljfifthharmoni might lose friend today know idc'\n",
      "1\t0.00\tb\"bixbersboca good morn lol realli fuckin dark 's gon na rain hard coupl minut\"\n",
      "1\t0.00\tb'asianmeerkat johncrossmirror far lfc fan make expert spot mental weak lack consist'\n",
      "1\t0.00\tb\"'m amsterdam guy wooo\"\n",
      "1\t0.00\tb'matjazsircelj stuff happen'\n",
      "1\t0.00\tb'nayakhk 2 indirag want judiciari commit govt polici want rbi commit know mani similar view'\n",
      "1\t0.00\tb'1robbeasley progress stone transfer chelsea'\n",
      "1\t0.00\tb'bryantduncan98 cours man nofx shit'\n",
      "1\t0.00\tb'kimblefrench morn kim sorri hear shed light issu look helen'\n",
      "1\t0.00\tb\"fcunitedmcr get close ca n't wait new season\"\n",
      "1\t0.00\tb'katelynland gz name =^.^='\n",
      "1\t0.00\tb'guess sleep list thing today'\n",
      "1\t0.00\tb'connelljess wan na loser like '\n",
      "1\t0.00\tb'poemporn hah .... thousand lie'\n",
      "1\t0.00\tb'flurish blxcknicotin wow look good isabella make wan na start work back'\n",
      "1\t0.00\tb'go room bc 3'\n",
      "1\t0.00\tb'gangesh_gugi think write'\n",
      "1\t0.00\tb'final went drive tonight'\n",
      "1\t0.00\tb'im go sleep goodnight'\n",
      "1\t0.00\tb\"abnormal_ana92 'm suppos ask buy album\"\n",
      "1\t0.00\tb'guess never realli identifi pharmaci verylaterealis iwishiknewbett'\n",
      "1\t0.00\tb'charg speaker final got earlier today'\n",
      "1\t0.00\tb\"babyytayy_ text wan na meet 'll get\"\n",
      "1\t0.00\tb'friend never talk ess-aych-eye-te anyon'\n",
      "1\t0.00\tb\"delafro_ unbearvbl yeah left one supposedli hurt bc 's right heart appar 'll fine good luck\"\n",
      "1\t0.00\tb\"niall follow fan 'm still without follow \"\n",
      "1\t0.00\tb'wendy1704 queen_of_orang need stress time'\n",
      "1\t0.00\tb\"igroundbreak could make foxi 's song version instrument form sound epic beat alon\"\n",
      "1\t0.00\tb'5g liker like fast'\n",
      "1\t0.00\tb'captaainmorgan okay'\n",
      "1\t0.00\tb\"greatestcooki hurt read peopl 's holiday work\"\n",
      "1\t0.00\tb'jedieconomist thought  like mate'\n",
      "1\t0.00\tb\"ignuk 1tbps4 wow prize delight eye pick win burst sing `` ps4 mine feel divin ''\"\n",
      "1\t0.00\tb'get readi ... ubericecream rekolacz messengerforaday http //t.co/dnrypnhad5'\n",
      "1\t0.00\tb'justinbieb daddi af ...'\n",
      "1\t0.00\tb'20 ghanton se light nahi'\n",
      "1\t0.00\tb\"one n't last long lol block alreadi must nice tweet hate limelite001\"\n",
      "1\t0.00\tb\"odellschwarzejg easy.get 5:30 go work come home bout 6 take care home famili therein 's 'll see\"\n",
      "1\t0.00\tb'ianeboyiest piti parti'\n",
      "1\t0.00\tb\"dominiquepirri unlatch shinonsai dominiqu 'm biggest fan like oh 'm england get fan sign\"\n",
      "1\t0.00\tb\"word anaturalwed amp thestoveroom big site 've got chang a-foot websit develop revamp\"\n",
      "1\t0.00\tb'anoushy_aliyan 8624810880khan aliapari86 ayeshakhanr riddamazhar confid confin u ur limit ...'\n",
      "1\t0.00\tb'danielnewman wish colorado'\n",
      "1\t0.00\tb'add snapchat yall give name'\n",
      "1\t0.00\tb\"fell asleep like 6:30 ca n't fall asleep two hour\"\n",
      "1\t0.00\tb'pakalupapito imma use next time'\n",
      "1\t0.00\tb'ob 11h v kino'\n",
      "1\t0.00\tb\"gone make tho posit day 're adult ca n't keep act like mom ..\"\n",
      "1\t0.00\tb'chicken caesar salad tad bland live freshmenublr'\n",
      "1\t0.00\tb'machado_man4 tashashukla cloydriv freedom hangout whoever want'\n",
      "1\t0.00\tb\"'m like\"\n",
      "1\t0.00\tb\"circuit500 yeah 've lost 's call day\"\n",
      "1\t0.00\tb'notjagath member   chanc chevindu'\n",
      "1\t0.00\tb'greggsoffici miss babi onemochaonelov'\n",
      "1\t0.00\tb'last class morn two week break'\n",
      "1\t0.00\tb'belittle4u may find confus simpl wolf'\n",
      "1\t0.00\tb'rinpunzel_ promot'\n",
      "1\t0.00\tb'hahahakumakichi yeah bird transform sword'\n",
      "1\t0.00\tb'kareemlshenawi even real word'\n",
      "1\t0.00\tb\"rickybaby321 's forward richard ... 's 'follow\"\n",
      "1\t0.00\tb'yup happen everywher peopl cant debat come name call http //t.co/drimwpj7on'\n",
      "1\t0.00\tb'need get work today ibiza birthdaymoneyforjesusjuic'\n",
      "1\t0.00\tb\"puppyshogun mistak happen man long get play game 'll happi\"\n",
      "1\t0.00\tb\"sleekstudioz ye 've alway selfish fuck\"\n",
      "1\t0.00\tb'man sing rain http //t.co/uj3mnlajmo'\n",
      "1\t0.00\tb'laguna see'\n",
      "1\t0.00\tb\"melfoster666 's true quit whenev want\"\n",
      "1\t0.00\tb'pop_ruth miss http //t.co/upxtepl57i'\n",
      "1\t0.00\tb\"demoorsophi hii follow pleas 'd like ask one thing dm \"\n",
      "1\t0.00\tb\"jenlawu 's disgust glad acc back hope get deserv\"\n",
      "1\t0.00\tb'alejandrinar32 okay'\n",
      "1\t0.00\tb'vadervanodin im wear mine dj next fri'\n",
      "1\t0.00\tb'dailybblif juliztwinsbb alreadi'\n",
      "1\t0.00\tb\"'m person workout eat healthi minut n't gain weight\"\n",
      "1\t0.00\tb'gm meet preciou soft ladi real friend mizz preciou mani like ... http //t.co/g9f2cwo6wz'\n",
      "1\t0.00\tb'everyon fuckin irrit'\n",
      "1\t0.00\tb'also mean imma go back twitter activ caus know everyon miss xd'\n",
      "1\t0.00\tb\"awad_gina phonicfm womenaward oh gina busi ca n't wait see hear xxx soproud\"\n",
      "1\t0.00\tb'cottypn paulpne70 trekkingpaul charlvdh fcpne pneshirley mandajohnston yvonneorr gem_pnefc kimramshead morn emma'\n",
      "1\t0.00\tb'love make two heart one'\n",
      "1\t0.00\tb'tsad close enought'\n",
      "1\t0.00\tb\"denisealicia_ salon bleach hair olaplex wo n't damag like\"\n",
      "1\t0.00\tb\"sbs_mtv  got7 let 's got7 fact\"\n",
      "1\t0.00\tb\"t'would tweetup without coleman_21 book sabrina_boat rest assur mbutlercoleshr book\"\n",
      "1\t0.00\tb'get home 4 wake 9'\n",
      "1\t0.00\tb'okkkk frend milt h break k bad'\n",
      "1\t0.00\tb'tangerinebean got bed yet'\n",
      "1\t0.00\tb\"pottorti awwww sige next time u know na punta kayo dun imma make sure na 'll go din\"\n",
      "1\t0.00\tb'lolarnav noooo didnt even know that actual type googl'\n",
      "1\t0.00\tb'found prompt http //t.co/f2hu2dalox'\n",
      "1\t0.00\tb'home alon'\n",
      "1\t0.00\tb'wayward pine later'\n",
      "1\t0.00\tb'blooodofoiympu johncrossmirror far lfc fan make expert spot mental weak lack consist'\n",
      "1\t0.00\tb'park get sunlight'\n",
      "1\t0.00\tb\"'m insecur tonight\"\n",
      "1\t0.00\tb'mom horror movi'\n",
      "1\t0.00\tb\"fvckl oh hell yeah 'll expect text next tuesday\"\n",
      "1\t0.00\tb'match day bitchessss real madrid vs man shitti'\n",
      "1\t0.00\tb'brianteeman gw1992 squash commit still make 200 commit'\n",
      "1\t0.00\tb\"first love wan na fuck late night think got nut v '' look ... http //t.co/8yhlcb16lf\"\n",
      "1\t0.00\tb'imynnx done yun'\n",
      "1\t0.00\tb'watch joe dirt 2'\n",
      "1\t0.00\tb'3hr .. tym prepar'\n",
      "1\t0.00\tb\"mcunleash ca n't sleep need tri lay bed bore\"\n",
      "1\t0.00\tb\"`` n't b call friend need '' ... http //t.co/lfszl8ubxt\"\n",
      "1\t0.00\tb'satbains1 congratul sat 3rd honorari degre everyon alac'\n",
      "1\t0.00\tb\"save money use good way money money 's better life fun nice day 11:11\"\n",
      "1\t0.00\tb'megalos_k like eye'\n",
      "1\t0.00\tb'grillo_23 sleepyjoe2 karnage90 richarddawkin dear person pleas studi embarrass urself entropi work 100 w/ evolut'\n",
      "1\t0.00\tb'anoth note found camden town ... work http //t.co/mdtjgcxopb'\n",
      "1\t0.00\tb\"wo n't even talk great\"\n",
      "1\t0.00\tb'neuer post onlin elfcosmet fix spray http //t.co/v0rm1bseij'\n",
      "1\t0.00\tb'class today train friend'\n",
      "1\t0.00\tb'one fall love choic  chanc ... '\n",
      "1\t0.00\tb'chordoverstreet seen uni .. talent http //t.co/enq3xuopsn'\n",
      "1\t0.00\tb'msarosh uff itna miss karhi thi ap p'\n",
      "1\t0.00\tb'mca money tell stori http //t.co/i5xs6tqe9w'\n",
      "1\t0.00\tb'katgraham_itali save live'\n",
      "1\t0.00\tb'jonscrazytweet still green tea blend flavor'\n",
      "1\t0.00\tb'chrismitchell91 sometim'\n",
      "1\t0.00\tb\"alexcarranza21 'm sorri ran friend rough day 'll tri stream saturday\"\n",
      "1\t0.00\tb'puff pastri egg tart hot fresh oven wan chai mtr http //t.co/hlkghwatjm'\n",
      "1\t0.00\tb'make alyssa rub tummi'\n",
      "1\t0.00\tb'bruneluni ive email regard cours queri'\n",
      "1\t0.00\tb'look fun snapchat gooffeanotter snapchat kiksex snapm lesbian instagram mpoint mugshot http //t.co/p4iqffswjp'\n",
      "1\t0.00\tb\"ca n't sleep much want love bug made 33 token never mind\"\n",
      "1\t0.00\tb'nasa nuf teas alreadi show us alien everyday seem get closer see alien cousin'\n",
      "1\t0.00\tb\"zoomtv kdrul mishrasugandha channel 's show planetbollywoodnew 's full epi avail onlin\"\n",
      "1\t0.00\tb'love airport'\n",
      "1\t0.00\tb\"swgguy hah .... n't say sorri ...\"\n",
      "1\t0.00\tb'chewy4cuti'\n",
      "1\t0.00\tb'im go bed love hailey milk crai sxxx cl runway look'\n",
      "1\t0.00\tb'feel someon share review work hard'\n",
      "1\t0.00\tb\"sabrinakean 's bad thing think weird face 's rad thank though\"\n",
      "1\t0.00\tb'`` make aliv make suffer make feel .. addict song alway sing karaok ..'\n",
      "1\t0.00\tb'ltsw gooooooo'\n",
      "1\t0.00\tb'5sostumblrx threw phone wall'\n",
      "1\t0.00\tb'aleeshajulia iapi_upd giant balloon want one'\n",
      "1\t0.00\tb'face time http //t.co/uoqastlhxo'\n",
      "1\t0.00\tb'thessanaomi thank much mom get sadcuddleashton ticket'\n",
      "1\t0.00\tb'wirral_in_it bevclack sharpeleven actual lol'\n",
      "1\t0.00\tb'aatishn carp diem mr nath'\n",
      "1\t0.00\tb\"11:11 meet michael hug tight talk tell 's import love much make smile\"\n",
      "1\t0.00\tb'long feel comfort im gon na wear want mother haha ... sound nice ...'\n",
      "1\t0.00\tb'time 2 parti http //t.co/hjnt6v40et'\n",
      "1\t0.00\tb'rblsport upgrad an sync plu done singl remot devic get folk'\n",
      "1\t0.00\tb'em__scott haha know mess'\n",
      "1\t0.00\tb\"yah know 'm good fake mah emot\"\n",
      "1\t0.00\tb'idk cant wait tonight panel peopl'\n",
      "1\t0.00\tb\"i'am elf hbu\"\n",
      "1\t0.00\tb'hswift65 roseoftheseale ps took 80-1 hope count'\n",
      "1\t0.00\tb'rememb someon  see feat charli puth wiz khalifa  http //t.co/vvaygndwnl'\n",
      "1\t0.00\tb\"'ve done today watch law amp order svu love sick\"\n",
      "1\t0.00\tb'ohhhtommyc aint never upload lol also found anoth vid u danc henri trap keep go fit journey tommi'\n",
      "1\t0.00\tb\"creativegossip ye think b stupid discuss abt wild card entri jst wait vivian 's perform\"\n",
      "1\t0.00\tb\"keab42 rang sick yesterday told 'd back today 'm better morn anyway day sunday\"\n",
      "1\t0.00\tb\"basementgalaxi ping 'm watch tl like hawk\"\n",
      "1\t0.00\tb'new potato garden hundr dig'\n",
      "1\t0.00\tb\"thevieweast cours 've cite academ paper feel 've arriv\"\n",
      "1\t0.00\tb\"'s still awak\"\n",
      "1\t0.00\tb'migogo true'\n",
      "1\t0.00\tb'fczbkk exactli'\n",
      "1\t0.00\tb'danieloconnel18 could say egg face'\n",
      "0\t1.00\tb'klm use pry/pv ..... wish could reliv day becom nyc/pv buy way commun nyc usa klm'\n",
      "0\t1.00\tb'live fam bam cough'\n",
      "0\t1.00\tb\"missalicebmbd 's sore alic\"\n",
      "0\t1.00\tb'infinityandbion funni mo'\n",
      "0\t1.00\tb\"quality_c n't ga\"\n",
      "0\t1.00\tb'thekelseeey awhhh ok ok see nalang class open hehe'\n",
      "0\t1.00\tb\"ryannhough imagin would shatter dream 'll let cooperativefood colleagu know ^sb\"\n",
      "0\t1.00\tb'3 day without talk bae'\n",
      "0\t1.00\tb\"nakakapikon yung nagbabasa ka ng blog comment info 's full peopl ask damn question answer post\"\n",
      "0\t1.00\tb'kevinperri lead caus cancer children five'\n",
      "0\t1.00\tb'rcdlccom hello info possibl interest jonatha close join beti great'\n",
      "0\t1.00\tb'lucyanne_l thank send premium write instrument howev dastardli swine stole envelop http //t.co/xne3cd6dvk'\n",
      "0\t1.00\tb'today  job anoth fack intel extra mega care get bent socket pin pcgame pcupgrad http //t.co/vejns9fbfn'\n",
      "0\t1.00\tb'sepanx one http //t.co/4hp6d3smwr'\n",
      "0\t1.00\tb'literalwt tya mind refollow'\n",
      "0\t1.00\tb'get bin osx/chrome/voiceov gt http //t.co/0bcva6yjwu'\n",
      "0\t1.00\tb' night devo xxx http //t.co/9ixtnybxlb'\n",
      "0\t1.00\tb'ssulstagram unfollow unni'\n",
      "0\t1.00\tb'nickiminaj mention south africa mani time song time come south africa'\n",
      "0\t1.00\tb'7pm friday dead'\n",
      "0\t1.00\tb'the_lie_lama back delhi'\n",
      "0\t1.00\tb'iamcharleigh_ fun'\n",
      "0\t1.00\tb'taylan247 know happen internet light connect drop thank kei'\n",
      "0\t1.00\tb'hanaaghzlli hanaaaa birthday ya allah sorri wish van jn tak tau happi birthday gorgeou'\n",
      "0\t1.00\tb'faith'\n",
      "0\t1.00\tb'last present receiv  haha basta http //t.co/xhdfobhn8n'\n",
      "0\t1.00\tb\"shop n't fun 're\"\n",
      "0\t1.00\tb'tiaramescudi girl quick'\n",
      "0\t1.00\tb'mani nasti narrow mind peopl'\n",
      "0\t1.00\tb'rcdeportivo hello info possibl interest jonatha close join beti'\n",
      "0\t1.00\tb'crepe 40  http //t.co/bazemhlhyl'\n",
      "0\t1.00\tb'jowaltham haha sound like  fun game'\n",
      "0\t1.00\tb'get hypixel want record grrrr thestruggleisr geek gamer gamer youtub'\n",
      "0\t1.00\tb'dream met karli kloss amp sweet amp want take like bunch goofi photo'\n",
      "0\t1.00\tb'care http //t.co/oe5id3cjir'\n",
      "0\t1.00\tb'hey christin moodi'\n",
      "0\t1.00\tb'throwback http //t.co/webri8gagm'\n",
      "0\t1.00\tb'ughponcong pretti'\n",
      "0\t1.00\tb'matt'\n",
      "0\t1.00\tb'deporsempre1 hello info possibl interest jonatha close join beti saludo'\n",
      "0\t1.00\tb'hey someon text'\n",
      "0\t1.00\tb'anoth dissapoint review http //t.co/hlnzzdw0tz bblogger makeup beauti bblogrt bbloggersunit femalebloggerrt'\n",
      "0\t1.00\tb\"harriep hi harri sorri 're work mast area caus intermitt servic work complet\"\n",
      "0\t1.00\tb'1 taxi servic 2 second schedul blog post 3 housework bust'\n",
      "0\t1.00\tb'nsc mat 0 tomorrow'\n",
      "0\t1.00\tb\"xsushi 're lana\"\n",
      "0\t1.00\tb'tip onlin 6/7 winner yesterday 1 goal nice 20/1 hope similar strike rate today http //t.co/ljeb3epzvt'\n",
      "0\t1.00\tb'phenomyoutub u prob fun david'\n",
      "0\t1.00\tb\"first two day kati summer 's back forth doctor suspect mening viral tonsil poorli girl \"\n",
      "0\t1.00\tb'babyy http //t.co/wx0gjcms9t'\n",
      "0\t1.00\tb'jaymcgui happi birthday pleas come back singapor  http //t.co/5liiqjqpbt'\n",
      "0\t1.00\tb'wolf_stack yeah look'\n",
      "0\t1.00\tb'vyenangel gosh cheaper malaysia worth 130 +ship'\n",
      "0\t1.00\tb'guy nooooo wow wow wow wow'\n",
      "0\t1.00\tb'ladyliaaxoxo never dedic anyth smh lol'\n",
      "0\t1.00\tb'deestrellado hello info possibl interest jonatha he close beti'\n",
      "0\t1.00\tb'develop releas good game school'\n",
      "0\t1.00\tb\"n't spoil http //t.co/xjgo5znsjh\"\n",
      "0\t1.00\tb'fav chees give migrin lt lt lt lt lt lt lt lt lt lt'\n",
      "0\t1.00\tb\"vgo__ oh do't think parcel come yodel hit follow amp dm track num chelsea\"\n",
      "0\t1.00\tb'oooooouch poor pinki toe  good thing work podiatrist wait morn go hurri morn'\n",
      "0\t1.00\tb\"gusto ko ng rodic 's someon share order one 's big\"\n",
      "0\t1.00\tb'21dadoongi ye hope next year'\n",
      "0\t1.00\tb'last episod realli intens kagami n kuroko forev lt 3'\n",
      "0\t1.00\tb\"elementaladam look amaz think saw meadowhal day actual unfortun weekend 'll work\"\n",
      "0\t1.00\tb'missin homeslic bday lt /333 wraithmedia http //t.co/1j3qyuttqg'\n",
      "0\t1.00\tb'fav emoticon right `` `` emoticon'\n",
      "0\t1.00\tb'iahmedmallick lightagayi aur ap bhi shamil ho'\n",
      "0\t1.00\tb'gambl3d hey sorri hear check servic statu page  http //t.co/xyurkksa60 gen'\n",
      "0\t1.00\tb'jjjaneel ye'\n",
      "0\t1.00\tb'time last week rout lovebox mixedgemsbeauti share day new collectionlov prod http //t.co/tchmtf8om4'\n",
      "0\t1.00\tb'need job learn drive fricken health put back'\n",
      "0\t1.00\tb'waniiamira ehem  haha ala yeke okay fun kk jumpa next time   maybe'\n",
      "0\t1.00\tb\"'ve shit bestfriend\"\n",
      "0\t1.00\tb\"rebelgirl1323 well tri someth keep mind bad thing 're alway\"\n",
      "0\t1.00\tb'remnantsofm oh build amp suit'\n",
      "0\t1.00\tb'krystalhost push client way host mayb better wait till next week'\n",
      "0\t1.00\tb'zombieham lot good comic event seem kid part school program'\n",
      "0\t1.00\tb'kyraavazquez u guy togeth'\n",
      "0\t1.00\tb\"sp3ctacl3 's friend\"\n",
      "0\t1.00\tb'use flexibl http //t.co/fc4nyvnfkb'\n",
      "0\t1.00\tb\"wajiyaamjad ok 'll write u everi week\"\n",
      "0\t1.00\tb'look like http //t.co/dgyngephuh'\n",
      "0\t1.00\tb\"isahurairah 's quot tweet\"\n",
      "0\t1.00\tb'yslzaint one http //t.co/s5dw4o9nvb'\n",
      "0\t1.00\tb\"time like style n't like http //t.co/jjsi8vscpl\"\n",
      "0\t1.00\tb'thomasleg account meant hit us lunch time'\n",
      "0\t1.00\tb'hammer right ...... look set day .....'\n",
      "0\t1.00\tb'hookuptrad u give 350 rihannapedia amp holyhardwel ill give u 350 350 rt anyth pl'\n",
      "0\t1.00\tb\"carterreynold hankgreen 're asian ummmm mayb ur film child pornographi slutsham fake suicid ect\"\n",
      "0\t1.00\tb'theresaninkspot well cooki better worth'\n",
      "0\t1.00\tb\"malcolminx lmao catch point watch weekli 'll ok make 700 amp 's 800 ep\"\n",
      "0\t1.00\tb\"angelicaorgan keep forget 're area want visit next time xx\"\n",
      "0\t1.00\tb'fun osaka super junior would love watch comeback stage though'\n",
      "0\t1.00\tb\"backtrack 3 hour .... 's time start actual util list\"\n",
      "0\t1.00\tb'pat jay'\n",
      "0\t1.00\tb'2 bach http //t.co/dprnccnezd'\n",
      "0\t1.00\tb'give work old staff come polic later court look anoth job bye bye'\n",
      "0\t1.00\tb'love warrior'\n",
      "0\t1.00\tb'wufanit tomorrow'\n",
      "0\t1.00\tb'bae_t whatev stil l young gt'\n",
      "0\t1.00\tb\"miss_j_hart staffrm well 'm p.41 amp 8 page note thu far 've read twice\"\n",
      "0\t1.00\tb'dem free tix big bang concert'\n",
      "0\t1.00\tb\"carter n't deserv hate n't deserv singl corn chip deserv two corn chip\"\n",
      "0\t1.00\tb'want sa bday ko ^_^ ^_^ lt 3'\n",
      "0\t1.00\tb'gztaehun ok amp w hahaahahahaha'\n",
      "0\t1.00\tb'soorjugn dare'\n",
      "0\t1.00\tb'good morn twitter friend hope amaz day freakin tire need sleep'\n",
      "0\t1.00\tb'wish twitter would support audio record would send yall snippet amaz'\n",
      "0\t1.00\tb'et brotherhood steel http //t.co/3qh5rrf0x'\n",
      "0\t1.00\tb\"master godfreyelfwick figur turn led gold 're doom meteoryan\"\n",
      "0\t1.00\tb'got excit micha rt fave tweet creep japhantrash'\n",
      "0\t1.00\tb'whose idea ave tripl busi lectur friday afternoon 2-5'\n",
      "0\t1.00\tb'mursano buy someth drank caus noth'\n",
      "0\t1.00\tb'wait love recuerda tanto bath'\n",
      "0\t1.00\tb'much await excit go bore friday weekend'\n",
      "0\t1.00\tb\"best spectat sail uk w'end bbc show highlight  fli  foil ac45 catamaran http //t.co/p4n34djjuw\"\n",
      "0\t1.00\tb'kathrynlnewton love babi sweet cinnamon best'\n",
      "0\t1.00\tb'peng bestfriend psygustokita'\n",
      "0\t1.00\tb'jaymcgui jaymcgui pleas notic men'\n",
      "0\t1.00\tb'ikoimoy watch us'\n",
      "0\t1.00\tb'regret regret'\n",
      "0\t1.00\tb\"j13ssk hi oh let 's take look pleas chat us http //t.co/otfgtfoeyf\"\n",
      "0\t1.00\tb'fave unfollow  itisfurni http //t.co/hovqloktig'\n",
      "0\t1.00\tb'zayn_come_back_we_miss_y lt 3 lt 3 much zaynmalik'\n",
      "0\t1.00\tb'lrt bambam like soda meant'\n",
      "0\t1.00\tb'1 http //t.co/ppuufiznb3'\n",
      "0\t1.00\tb'_jazdorothi cheer'\n",
      "0\t1.00\tb'happi birthday be mahhriel_114 enjoy day beb love miss sooooo muchi see soon sana  http //t.co/c5oobr6ov9'\n",
      "0\t1.00\tb'cuti http //t.co/xlvw7xaysi'\n",
      "0\t1.00\tb\"uber_blr say 'no icecream avail\"\n",
      "0\t1.00\tb\"naw deep dream novel hope 'd invis page beast scuttl around 's worm track http //t.co/fethzt8bf\"\n",
      "0\t1.00\tb'worri'\n",
      "0\t1.00\tb'jessica call quit power ab 5:15'\n",
      "0\t1.00\tb\"msmeghanmakeup hope 're fun vidcon rlli bum couldnt go love sunshin \"\n",
      "0\t1.00\tb'last night good '\n",
      "0\t1.00\tb'princeofrnbzjm follow'\n",
      "0\t1.00\tb'belov grandmoth http //t.co/wt4oxq5xcf'\n",
      "0\t1.00\tb'happi princess today sigh get work done x purpl princess edit'\n",
      "0\t1.00\tb\"babeeee 're demn hotaisndonwyvauwjoqhsjsnaihsuswtf http //t.co/kwwv5grny7\"\n",
      "0\t1.00\tb'think need 2 year train beat record sia'\n",
      "0\t1.00\tb'niram ya geng fikri anna 6 other tirtagangga hotel  http //t.co/46fl7vhil3'\n",
      "0\t1.00\tb'riprishikeshwari soul rest peac'\n",
      "0\t1.00\tb\"the5ballov radio702 's challeng though pleas check fb page entri rather substitut thank\"\n",
      "0\t1.00\tb'deviousliz awh  stay '\n",
      "0\t1.00\tb'last full night greec opu inner pleasur http //t.co/poglaqg9sj'\n",
      "0\t1.00\tb'bobwilson1955 ye bbq attend'\n",
      "0\t1.00\tb'lol got split hard lol singl singl split collat spilt singl singl spilt singl split singl singl collat'\n",
      "0\t1.00\tb'hope feel better jay'\n",
      "0\t1.00\tb'2baconil ye mani time quitkarwaoyaaro'\n",
      "0\t1.00\tb'parisianski lest gp get ^_^'\n",
      "0\t1.00\tb'seolhyun isnt first ep she film drama'\n",
      "0\t1.00\tb'fellow morn owl sure notic sun alreadi rise bit later ... sign winter come http //t.co/4ou2eyph8a'\n",
      "0\t1.00\tb'hot'\n",
      "0\t1.00\tb'hi life'\n",
      "0\t1.00\tb'pledis_17 mood whole day'\n",
      "0\t1.00\tb'almost done park rec lol'\n",
      "0\t1.00\tb\"lplatten hi 're sorri hear log http //t.co/76lov3arfc 'll abl see inform need\"\n",
      "0\t1.00\tb'gotdamn http //t.co/kkpdlqz2f4'\n",
      "0\t1.00\tb'ozzyosbourn slash first saw gnr open alic cooper famou parti backstag alic r slash'\n",
      "0\t1.00\tb'roguebassjst love u fuck know'\n",
      "0\t1.00\tb'qjqj_kwon weird ^^'\n",
      "0\t1.00\tb'sr. financi analyst expedia inc. bellevu wa http //t.co/ktknmhvwci financ expediajob job job hire'\n",
      "0\t1.00\tb'imdanielpadilla hi love u'\n",
      "0\t1.00\tb'nooooooooo last day today'\n",
      "0\t1.00\tb'go home blue back monday hiby social action plan herts1617 shareyoursumm http //t.co/7pjj9q5v7z'\n",
      "0\t1.00\tb'pro soccer play would cool'\n",
      "0\t1.00\tb\"itvcentr midland ye thank depress weather forecast word 'rain mention sever time\"\n",
      "0\t1.00\tb\"emergingrecruit _roblong 'll old one day .....\"\n",
      "0\t1.00\tb'knowwwww http //t.co/khkzmes7kf'\n",
      "0\t1.00\tb'pengen box golds_indonesia  http //t.co/qxg4una4fn'\n",
      "0\t1.00\tb'tanyastan4nicki girlll u hear abt possibl tsunami us west indi im island im afraid lol'\n",
      "0\t1.00\tb'bacheloretteabc kaitlynbristow would love see w/nick pick one waist tv time amp never work anyway'\n",
      "0\t1.00\tb'hulk hogan news racial tirad thought couldnt stoop lower role thunder paradis http //t.co/vd0z6x8t8g'\n",
      "0\t1.00\tb'aylesburyowl ljam185 facad democraci drop quickli'\n",
      "0\t1.00\tb\"esp__132 oh realli saw gif post seen realli happi mayb 's peopl 'd happi naruhina\"\n",
      "0\t1.00\tb'loyalistofsoshi mcountdown pre-vot begin 5th place'\n",
      "0\t1.00\tb\"flrsbea wow 's great 's usernam\"\n",
      "0\t1.00\tb'like join marin'\n",
      "0\t1.00\tb'_mekele_ yogscastlewi nope best place get first pre-ord amazon http //t.co/fzrci1xonv'\n",
      "0\t1.00\tb\"denoct thank lack time problem n't\"\n",
      "0\t1.00\tb'ur cat super nice cuddli suddenli scratch tri bite like trust anymor'\n",
      "0\t1.00\tb'hit thumb'\n",
      "0\t1.00\tb'laraibmufc follow back'\n",
      "0\t1.00\tb'dream http //t.co/ozdmhnqbsu'\n",
      "0\t1.00\tb'last day beach .... http //t.co/oqo9f7fqs9'\n",
      "0\t1.00\tb\"ranaptx_ lead us 're gone\"\n",
      "0\t1.00\tb'im get olli tweet notif'\n",
      "0\t1.00\tb'mattelya way'\n",
      "0\t1.00\tb\"earth assum would n't rain london like influenc overal warm dri past week continent europ\"\n",
      "0\t1.00\tb'dongvvoo1122  r u sure u want b r u h bare surviv w tank top'\n",
      "0\t1.00\tb'blatblatklang ummmm ok new develop'\n"
     ]
    }
   ],
   "source": [
    "# Some error analysis done for you\n",
    "print('Truth Predicted Tweet')\n",
    "for x, y in zip(test_x, test_y):\n",
    "    y_hat = naive_bayes_predict(x, logprior, loglikelihood)\n",
    "    if y != (np.sign(y_hat) > 0):\n",
    "        print('%d\\t%0.2f\\t%s' % (y, np.sign(y_hat) > 0, ' '.join(\n",
    "            process_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5705368957188433\n"
     ]
    }
   ],
   "source": [
    "# Test with your own tweet - feel free to modify `my_tweet`\n",
    "my_tweet = 'I am happy because I am learning :)'\n",
    "\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

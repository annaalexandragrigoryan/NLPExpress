{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autocorrection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import w1_unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(file_name):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        A file_name which is found in your current directory. You just have to read it in. \n",
    "    Output: \n",
    "        words: a list containing all the words in the corpus (text file you read) in lower case. \n",
    "    \"\"\"\n",
    "    words = []\n",
    "\n",
    "    \n",
    "    # Open the file and read its contents into a string variable\n",
    "    with open(file_name, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Convert all letters to lower case\n",
    "    text = text.lower()\n",
    "    words = re.findall(r'\\w+', text)\n",
    "\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first ten words in the text are: \n",
      "['o', 'for', 'a', 'muse', 'of', 'fire', 'that', 'would', 'ascend', 'the']\n",
      "There are 6116 unique words in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "#DO NOT MODIFY THIS CELL\n",
    "word_l = process_data('./data/shakespeare.txt')\n",
    "vocab = set(word_l)  # this will be your new vocabulary\n",
    "print(f\"The first ten words in the text are: \\n{word_l[0:10]}\")\n",
    "print(f\"There are {len(vocab)} unique words in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(word_l):\n",
    "    '''\n",
    "    Input:\n",
    "        word_l: a set of words representing the corpus. \n",
    "    Output:\n",
    "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
    "    '''\n",
    "    \n",
    "    word_count_dict = {}  # fill this with word counts\n",
    "    \n",
    "    # Iterate through the list of words\n",
    "    for word in word_l:\n",
    "        # Update the word count in the dictionary\n",
    "        if word in word_count_dict:\n",
    "            word_count_dict[word] += 1\n",
    "        else:\n",
    "            word_count_dict[word] = 1\n",
    "\n",
    "    return word_count_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6116 key values pairs\n",
      "The count for the word 'thee' is 240\n"
     ]
    }
   ],
   "source": [
    "word_count_dict = get_count(word_l)\n",
    "print(f\"There are {len(word_count_dict)} key values pairs\")\n",
    "print(f\"The count for the word 'thee' is {word_count_dict.get('thee',0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3 GRADED FUNCTION: get_probs\n",
    "def get_probs(word_count_dict):\n",
    "    '''\n",
    "    Input:\n",
    "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
    "    Output:\n",
    "        probs: A dictionary where keys are the words and the values are the probability that a word will occur. \n",
    "    '''\n",
    "    probs = {}  # initialize the dictionary to store probabilities\n",
    "    \n",
    "    \n",
    "    # Calculate the total count of words in the corpus\n",
    "    total_words = sum(word_count_dict.values())\n",
    "    \n",
    "    # Calculate the probability for each word\n",
    "    for word, count in word_count_dict.items():\n",
    "        probs[word] = count / total_words\n",
    "        \n",
    "    return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of probs is 6116\n",
      "P('thee') is 0.0045\n"
     ]
    }
   ],
   "source": [
    "probs = get_probs(word_count_dict)\n",
    "print(f\"Length of probs is {len(probs)}\")\n",
    "print(f\"P('thee') is {probs['thee']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - String Manipulations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the string/word for which you will generate all possible words \n",
    "                in the vocabulary which have 1 missing character\n",
    "    Output:\n",
    "        delete_l: a list of all possible strings obtained by deleting 1 character from word\n",
    "    '''\n",
    "    \n",
    "    delete_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n",
    "    delete_l = [L + R[1:] for L, R in split_l if R]\n",
    "\n",
    "    if verbose: print(f\"input word {word}, \\nsplit_l = {split_l}, \\ndelete_l = {delete_l}\")\n",
    "\n",
    "    return  delete_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word cans, \n",
      "split_l = [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's')], \n",
      "delete_l = ['ans', 'cns', 'cas', 'can']\n"
     ]
    }
   ],
   "source": [
    "delete_word_l = delete_letter(word=\"cans\",\n",
    "                        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "def switch_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: input string\n",
    "     Output:\n",
    "        switches: a list of all possible strings with one adjacent charater switched\n",
    "    ''' \n",
    "    \n",
    "    switch_l = []\n",
    "    split_l = []\n",
    "\n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n",
    "    switch_l = [L + R[1] + R[0] + R[2:] for L, R in split_l if len(R) >= 2]\n",
    "    \n",
    "    \n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nswitch_l = {switch_l}\") \n",
    "    \n",
    "    return switch_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word = eta \n",
      "split_l = [('', 'eta'), ('e', 'ta'), ('et', 'a')] \n",
      "switch_l = ['tea', 'eat']\n"
     ]
    }
   ],
   "source": [
    "switch_word_l = switch_letter(word=\"eta\",\n",
    "                         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        replaces: a list of all possible strings where we replaced one letter from the original word. \n",
    "    ''' \n",
    "    \n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    \n",
    "    replace_set = set()\n",
    "    split_l = []\n",
    "    \n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n",
    "    for L, R in split_l:\n",
    "        for c in letters:\n",
    "            if len(R) >= 1:\n",
    "                replace_set.add(L + c + R[1:])\n",
    "    replace_set.discard(word)\n",
    "    # turn the set back into a list and sort it, for easier viewing\n",
    "    replace_l = sorted(list(replace_set))\n",
    "    \n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nreplace_l {replace_l}\")   \n",
    "    return replace_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word = can \n",
      "split_l = [('', 'can'), ('c', 'an'), ('ca', 'n')] \n",
      "replace_l ['aan', 'ban', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan']\n"
     ]
    }
   ],
   "source": [
    "replace_l = replace_letter(word='can',\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        inserts: a set of all possible strings with one new letter inserted at every offset\n",
    "    ''' \n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    insert_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    \n",
    "    # Iterate through every position (offset) in the word\n",
    "    for i in range(len(word) + 1):\n",
    "        # Split the word into left and right parts at the current offset\n",
    "        left_part = word[:i]\n",
    "        right_part = word[i:]\n",
    "        \n",
    "        # For every letter in the alphabet, insert it between the left and right parts\n",
    "        for letter in letters:\n",
    "            new_word = left_part + letter + right_part\n",
    "            insert_l.append(new_word)\n",
    "            split_l.append((left_part, letter, right_part))  # For verbose output\n",
    "    \n",
    "    \n",
    "    if verbose: print(f\"Input word {word} \\nsplit_l = {split_l} \\ninsert_l = {insert_l}\")\n",
    "    \n",
    "    return insert_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word at \n",
      "split_l = [('', 'a', 'at'), ('', 'b', 'at'), ('', 'c', 'at'), ('', 'd', 'at'), ('', 'e', 'at'), ('', 'f', 'at'), ('', 'g', 'at'), ('', 'h', 'at'), ('', 'i', 'at'), ('', 'j', 'at'), ('', 'k', 'at'), ('', 'l', 'at'), ('', 'm', 'at'), ('', 'n', 'at'), ('', 'o', 'at'), ('', 'p', 'at'), ('', 'q', 'at'), ('', 'r', 'at'), ('', 's', 'at'), ('', 't', 'at'), ('', 'u', 'at'), ('', 'v', 'at'), ('', 'w', 'at'), ('', 'x', 'at'), ('', 'y', 'at'), ('', 'z', 'at'), ('a', 'a', 't'), ('a', 'b', 't'), ('a', 'c', 't'), ('a', 'd', 't'), ('a', 'e', 't'), ('a', 'f', 't'), ('a', 'g', 't'), ('a', 'h', 't'), ('a', 'i', 't'), ('a', 'j', 't'), ('a', 'k', 't'), ('a', 'l', 't'), ('a', 'm', 't'), ('a', 'n', 't'), ('a', 'o', 't'), ('a', 'p', 't'), ('a', 'q', 't'), ('a', 'r', 't'), ('a', 's', 't'), ('a', 't', 't'), ('a', 'u', 't'), ('a', 'v', 't'), ('a', 'w', 't'), ('a', 'x', 't'), ('a', 'y', 't'), ('a', 'z', 't'), ('at', 'a', ''), ('at', 'b', ''), ('at', 'c', ''), ('at', 'd', ''), ('at', 'e', ''), ('at', 'f', ''), ('at', 'g', ''), ('at', 'h', ''), ('at', 'i', ''), ('at', 'j', ''), ('at', 'k', ''), ('at', 'l', ''), ('at', 'm', ''), ('at', 'n', ''), ('at', 'o', ''), ('at', 'p', ''), ('at', 'q', ''), ('at', 'r', ''), ('at', 's', ''), ('at', 't', ''), ('at', 'u', ''), ('at', 'v', ''), ('at', 'w', ''), ('at', 'x', ''), ('at', 'y', ''), ('at', 'z', '')] \n",
      "insert_l = ['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', 'aat', 'abt', 'act', 'adt', 'aet', 'aft', 'agt', 'aht', 'ait', 'ajt', 'akt', 'alt', 'amt', 'ant', 'aot', 'apt', 'aqt', 'art', 'ast', 'att', 'aut', 'avt', 'awt', 'axt', 'ayt', 'azt', 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz']\n",
      "Number of strings output by insert_letter('at') is 78\n"
     ]
    }
   ],
   "source": [
    "insert_l = insert_letter('at', True)\n",
    "print(f\"Number of strings output by insert_letter('at') is {len(insert_l)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Combining the Edits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_one_letter(word, allow_switches=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        word: the string/word for which we will generate all possible words that are one edit away.\n",
    "    Output:\n",
    "        edit_one_set: a set of words with one possible edit. Please return a set and not a list.\n",
    "    \"\"\"\n",
    "    \n",
    "    edit_one_set = set()\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    \n",
    "\n",
    "    \n",
    "    # Deletion\n",
    "    for i in range(len(word)):\n",
    "        deletion = word[:i] + word[i+1:]\n",
    "        edit_one_set.add(deletion)\n",
    "    \n",
    "    # Transposition (switching adjacent letters)\n",
    "    if allow_switches:\n",
    "        for i in range(len(word) - 1):\n",
    "            transposition = word[:i] + word[i+1] + word[i] + word[i+2:]\n",
    "            edit_one_set.add(transposition)\n",
    "    \n",
    "    # Alteration (changing a letter)\n",
    "    for i in range(len(word)):\n",
    "        for letter in letters:\n",
    "            alteration = word[:i] + letter + word[i+1:]\n",
    "            edit_one_set.add(alteration)\n",
    "    \n",
    "    # Insertion\n",
    "    for i in range(len(word) + 1):\n",
    "        for letter in letters:\n",
    "            insertion = word[:i] + letter + word[i:]\n",
    "            edit_one_set.add(insertion)\n",
    "    \n",
    "\n",
    "    edit_one_set.discard(word)\n",
    "\n",
    "    return edit_one_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word at \n",
      "edit_one_l \n",
      "['a', 'aa', 'aat', 'ab', 'abt', 'ac', 'act', 'ad', 'adt', 'ae', 'aet', 'af', 'aft', 'ag', 'agt', 'ah', 'aht', 'ai', 'ait', 'aj', 'ajt', 'ak', 'akt', 'al', 'alt', 'am', 'amt', 'an', 'ant', 'ao', 'aot', 'ap', 'apt', 'aq', 'aqt', 'ar', 'art', 'as', 'ast', 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz', 'au', 'aut', 'av', 'avt', 'aw', 'awt', 'ax', 'axt', 'ay', 'ayt', 'az', 'azt', 'bat', 'bt', 'cat', 'ct', 'dat', 'dt', 'eat', 'et', 'fat', 'ft', 'gat', 'gt', 'hat', 'ht', 'iat', 'it', 'jat', 'jt', 'kat', 'kt', 'lat', 'lt', 'mat', 'mt', 'nat', 'nt', 'oat', 'ot', 'pat', 'pt', 'qat', 'qt', 'rat', 'rt', 'sat', 'st', 't', 'ta', 'tat', 'tt', 'uat', 'ut', 'vat', 'vt', 'wat', 'wt', 'xat', 'xt', 'yat', 'yt', 'zat', 'zt']\n",
      "\n",
      "The type of the returned object should be a set <class 'set'>\n",
      "Number of outputs from edit_one_letter('at') is 129\n"
     ]
    }
   ],
   "source": [
    "tmp_word = \"at\"\n",
    "tmp_edit_one_set = edit_one_letter(tmp_word)\n",
    "# turn this into a list to sort it, in order to view it\n",
    "tmp_edit_one_l = sorted(list(tmp_edit_one_set))\n",
    "\n",
    "print(f\"input word {tmp_word} \\nedit_one_l \\n{tmp_edit_one_l}\\n\")\n",
    "print(f\"The type of the returned object should be a set {type(tmp_edit_one_set)}\")\n",
    "print(f\"Number of outputs from edit_one_letter('at') is {len(edit_one_letter('at'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-2'></a>\n",
    "### 3.2 - Edit Two Letters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_two_letters(word, allow_switches = True):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        edit_two_set: a set of strings with all possible two edits\n",
    "    '''\n",
    "    \n",
    "    edit_two_set = set()\n",
    "    \n",
    "\n",
    "    # Generate one-edit-away words for the input word\n",
    "    one_edit_set = edit_one_letter(word, allow_switches)\n",
    "    \n",
    "    # Generate one-edit-away words for each word in the one_edit_set\n",
    "    for edited_word in one_edit_set:\n",
    "        edit_two_set.update(edit_one_letter(edited_word, allow_switches))\n",
    "        \n",
    "    \n",
    "    # return this as a set instead of a list\n",
    "    return set(edit_two_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of strings with edit distance of two: 2654\n",
      "First 10 strings ['', 'a', 'aa', 'aaa', 'aab', 'aac', 'aad', 'aae', 'aaf', 'aag']\n",
      "Last 10 strings ['zv', 'zva', 'zw', 'zwa', 'zx', 'zxa', 'zy', 'zya', 'zz', 'zza']\n",
      "The data type of the returned object should be a set <class 'set'>\n",
      "Number of strings that are 2 edit distances from 'at' is 7154\n"
     ]
    }
   ],
   "source": [
    "tmp_edit_two_set = edit_two_letters(\"a\")\n",
    "tmp_edit_two_l = sorted(list(tmp_edit_two_set))\n",
    "print(f\"Number of strings with edit distance of two: {len(tmp_edit_two_l)}\")\n",
    "print(f\"First 10 strings {tmp_edit_two_l[:10]}\")\n",
    "print(f\"Last 10 strings {tmp_edit_two_l[-10:]}\")\n",
    "print(f\"The data type of the returned object should be a set {type(tmp_edit_two_set)}\")\n",
    "print(f\"Number of strings that are 2 edit distances from 'at' is {len(edit_two_letters('at'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-3'></a>\n",
    "### 3.3 - Suggest Spelling Suggestions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['a', 'b']\n",
      "['Most', 'Likely']\n",
      "['least', 'of', 'all']\n"
     ]
    }
   ],
   "source": [
    "# example of logical operation on lists or sets\n",
    "print( [] and [\"a\",\"b\"] )\n",
    "print( [] or [\"a\",\"b\"] )\n",
    "#example of Short circuit behavior\n",
    "val1 =  [\"Most\",\"Likely\"] or [\"Less\",\"so\"] or [\"least\",\"of\",\"all\"]  # selects first, does not evalute remainder\n",
    "print(val1)\n",
    "val2 =  [] or [] or [\"least\",\"of\",\"all\"] # continues evaluation until there is a non-empty list\n",
    "print(val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrections(word, probs, vocab, n=2, verbose = False):\n",
    "    '''\n",
    "    Input: \n",
    "        word: a user entered string to check for suggestions\n",
    "        probs: a dictionary that maps each word to its probability in the corpus\n",
    "        vocab: a set containing all the vocabulary\n",
    "        n: number of possible word corrections you want returned in the dictionary\n",
    "    Output: \n",
    "        n_best: a list of tuples with the most probable n corrected words and their probabilities.\n",
    "    '''\n",
    "    \n",
    "    suggestions = []\n",
    "    n_best = []\n",
    "    \n",
    "    #Step 1: create suggestions as described above    \n",
    "    suggestions = None\n",
    "                    \n",
    "    #Step 2: determine probability of suggestions\n",
    "    \n",
    "    #Step 3: Get all your best words and return the most probable top n_suggested words as n_best\n",
    "    \n",
    "    n_best = None\n",
    "    \n",
    "    \n",
    "    if verbose: print(\"entered word = \", word, \"\\nsuggestions = \", suggestions)\n",
    "\n",
    "    return n_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrections(word, probs, vocab, n=2, verbose=False):\n",
    "    '''\n",
    "    Input: \n",
    "        word: a user entered string to check for suggestions\n",
    "        probs: a dictionary that maps each word to its probability in the corpus\n",
    "        vocab: a set containing all the vocabulary\n",
    "        n: number of possible word corrections you want returned in the dictionary\n",
    "    Output: \n",
    "        n_best: a list of tuples with the most probable n corrected words and their probabilities.\n",
    "    '''\n",
    "    \n",
    "    suggestions = []\n",
    "    n_best = []\n",
    "    \n",
    "    \n",
    "    # Step 1: Create suggestions by generating possible edits\n",
    "    edit_one = edit_one_letter(word)\n",
    "    #edit_two = edit_two_letters(word)\n",
    "    edit_two = []\n",
    "    # Combine both sets of edits\n",
    "    candidate_words = edit_one.union(edit_two)\n",
    "    \n",
    "    # Filter candidate words to include only those in the vocabulary\n",
    "    suggestions = candidate_words.intersection(vocab)\n",
    "    \n",
    "    # Step 2: Determine the probability of suggestions\n",
    "    suggestions_with_probs = {word: probs[word] for word in suggestions}\n",
    "    \n",
    "    # Step 3: Get the best n suggested words based on their probabilities\n",
    "    n_best = sorted(suggestions_with_probs.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "    \n",
    "    \n",
    "    if verbose: print(\"entered word =\", word, \"\\nsuggestions =\", n_best)\n",
    "\n",
    "    return n_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered word = dys \n",
      "suggestions = [('days', 0.0004103405826836274), ('dye', 1.865184466743761e-05)]\n",
      "word 0: days, probability 0.000410\n",
      "word 1: dye, probability 0.000019\n",
      "data type of corrections <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Test your implementation - feel free to try other words in my word\n",
    "my_word = 'dys' \n",
    "tmp_corrections = get_corrections(my_word, probs, vocab, 2, verbose=True) # keep verbose=True\n",
    "for i, word_prob in enumerate(tmp_corrections):\n",
    "    print(f\"word {i}: {word_prob[0]}, probability {word_prob[1]:.6f}\")\n",
    "\n",
    "# CODE REVIEW COMMENT: using \"tmp_corrections\" insteads of \"cors\". \"cors\" is not defined\n",
    "print(f\"data type of corrections {type(tmp_corrections)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Minimum Edit Distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def min_edit_distance(source, target, ins_cost=1, del_cost=1, rep_cost=2):\n",
    "    '''\n",
    "    Input: \n",
    "        source: a string corresponding to the string you are starting with\n",
    "        target: a string corresponding to the string you want to end with\n",
    "        ins_cost: an integer setting the insert cost\n",
    "        del_cost: an integer setting the delete cost\n",
    "        rep_cost: an integer setting the replace cost\n",
    "    Output:\n",
    "        D: a matrix of len(source)+1 by len(target)+1 containing minimum edit distances\n",
    "        med: the minimum edit distance (med) required to convert the source string to the target\n",
    "    '''\n",
    "    m = len(source) \n",
    "    n = len(target) \n",
    "    # Initialize cost matrix with zeros and dimensions (m+1,n+1)\n",
    "    D = np.zeros((m+1, n+1), dtype=int) \n",
    "    \n",
    "    \n",
    "    # Fill in column 0, from row 1 to row m, both inclusive\n",
    "    for row in range(1, m+1):\n",
    "        D[row, 0] = D[row-1, 0] + del_cost\n",
    "        \n",
    "    # Fill in row 0, for all columns from 1 to n, both inclusive\n",
    "    for col in range(1, n+1):\n",
    "        D[0, col] = D[0, col-1] + ins_cost\n",
    "        \n",
    "    # Loop through row 1 to row m, both inclusive\n",
    "    for row in range(1, m+1):\n",
    "        \n",
    "        # Loop through column 1 to column n, both inclusive\n",
    "        for col in range(1, n+1):\n",
    "            \n",
    "            # Intialize r_cost to the 'replace' cost that is passed into this function\n",
    "            r_cost = rep_cost\n",
    "            \n",
    "            # Check to see if source character at the previous row\n",
    "            # matches the target character at the previous column\n",
    "            if source[row-1] == target[col-1]:\n",
    "                # Update the replacement cost to 0 if source and target are the same\n",
    "                r_cost = 0\n",
    "                \n",
    "            # Update the cost at row, col based on previous entries in the cost matrix\n",
    "            # Refer to the equation calculate for D[i,j] (the minimum of three calculated costs)\n",
    "            D[row, col] = min(D[row-1, col] + del_cost,        # Deletion\n",
    "                              D[row, col-1] + ins_cost,        # Insertion\n",
    "                              D[row-1, col-1] + r_cost)        # Replacement\n",
    "            \n",
    "    # Set the minimum edit distance with the cost found at row m, column n \n",
    "    med = D[m, n]\n",
    "    \n",
    "    \n",
    "    return D, med\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum edits:  4 \n",
      "\n",
      "   #  s  t  a  y\n",
      "#  0  1  2  3  4\n",
      "p  1  2  3  4  5\n",
      "l  2  3  4  5  6\n",
      "a  3  4  5  4  5\n",
      "y  4  5  6  5  4\n"
     ]
    }
   ],
   "source": [
    "source =  'play'\n",
    "target = 'stay'\n",
    "matrix, min_edits = min_edit_distance(source, target)\n",
    "print(\"minimum edits: \",min_edits, \"\\n\")\n",
    "idx = list('#' + source)\n",
    "cols = list('#' + target)\n",
    "df = pd.DataFrame(matrix, index=idx, columns= cols)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Results:**  \n",
    "\n",
    "```CPP\n",
    "minimum edits:  4\n",
    "    \n",
    "   #  s  t  a  y\n",
    "#  0  1  2  3  4\n",
    "p  1  2  3  4  5\n",
    "l  2  3  4  5  6\n",
    "a  3  4  5  4  5\n",
    "y  4  5  6  5  4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum edits:  3 \n",
      "\n",
      "   #  n  e  a  r\n",
      "#  0  1  2  3  4\n",
      "e  1  2  1  2  3\n",
      "e  2  3  2  3  4\n",
      "r  3  4  3  4  3\n"
     ]
    }
   ],
   "source": [
    "source =  'eer'\n",
    "target = 'near'\n",
    "matrix, min_edits = min_edit_distance(source, target)\n",
    "print(\"minimum edits: \",min_edits, \"\\n\")\n",
    "idx = list(source)\n",
    "idx.insert(0, '#')\n",
    "cols = list(target)\n",
    "cols.insert(0, '#')\n",
    "df = pd.DataFrame(matrix, index=idx, columns= cols)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Results**  \n",
    "```CPP\n",
    "minimum edits:  3 \n",
    "\n",
    "   #  n  e  a  r\n",
    "#  0  1  2  3  4\n",
    "e  1  2  1  2  3\n",
    "e  2  3  2  3  4\n",
    "r  3  4  3  4  3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test several of our routines at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"eer\"\n",
    "targets = edit_one_letter(source,allow_switches = False)  #disable switches since min_edit_distance does not include them\n",
    "for t in targets:\n",
    "    _, min_edits = min_edit_distance(source, t,1,1,1)  # set ins, del, sub costs all to one\n",
    "    if min_edits != 1: print(source, t, min_edits)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
